{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation & transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "print('Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(1.),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset1 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainset2 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform1)\n",
    "dataset = data.ConcatDataset([trainset1, trainset2])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size= 128, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size= 128, shuffle=False, num_workers=4)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        #zeroing mask\n",
    "        for k, v in net.state_dict().items():\n",
    "            if 'conv' in k:\n",
    "                checkpoint[k] = v.cuda() * masks[k]\n",
    "        net.load_state_dict(checkpoint)\n",
    "        \n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print(\"Finetune epoch: {:d} Acc: {:.3f} ({:d}/{:d})\".format(epoch, 100.*correct/total, correct, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "#     net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    accuracy = 100.*correct/total\n",
    "    print(\"Test Acc: {:.3f} ({:d}/{:d})\".format(accuracy, correct, total))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prune a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_layer(in_weight, prune_ratio):\n",
    "    tres = -1e-4\n",
    "    mask = (abs (in_weight) > tres)\n",
    "    rat = 0\n",
    "    while rat < np.array(prune_ratio):     \n",
    "        mask = (abs (in_weight) > tres)\n",
    "        TAR = float(mask.sum())/float(in_weight.nelement())\n",
    "        rat = 1.0 - TAR\n",
    "        tres += (1e-5)\n",
    "    \n",
    "    pruned_weight = in_weight*mask.float()\n",
    "    \n",
    "\n",
    "    return pruned_weight, mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prune_layer(in_weight, prune_ratio):\n",
    "#     mask = torch.ones_like(in_weight)\n",
    "#     mask[:, :int(mask.shape[1]*prune_ratio), :, :] = 0\n",
    "#     pruned_weight = mask*in_weight\n",
    "#     return pruned_weight, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prune network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(prune_ratio):\n",
    "    masks = {}\n",
    "\n",
    "    for k, v in net.state_dict().items():\n",
    "        if 'conv' in k:\n",
    "            #print(\"pruning layer:\", k)\n",
    "            weights=v\n",
    "            weights, masks[k] = prune_layer(weights, prune_ratio)\n",
    "            checkpoint[k] = weights\n",
    "    net.load_state_dict(checkpoint)\n",
    "    \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print out network sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sparsity():\n",
    "    num_el = 0\n",
    "    num_zero = 0\n",
    "    for k, v in net.state_dict().items():\n",
    "        if 'conv' in k:\n",
    "            num_el += v.numel()\n",
    "            num_zero+=(v==0).sum().cpu().numpy()\n",
    "#     print(f\"num: {num_el} {num_zero}\")\n",
    "    sparsity = 100.*num_zero/num_el\n",
    "    print(\"Sparsity: {:.3f}%\".format(sparsity))\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing model and loading original checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model..\n",
      "Resuming from checkpoint..\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model\n",
    "print('Building model..')\n",
    "net = model.resnet18() \n",
    "\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "# Load weights that TA provided\n",
    "print('Resuming from checkpoint..')\n",
    "checkpoint_path = \"./checkpoint/resnet18_pruned.t7\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "net.load_state_dict(checkpoint)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See the original accuracy and sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 88.670 (8867/10000)\n",
      "Sparsity: 0.000%\n"
     ]
    }
   ],
   "source": [
    "_ = test()\n",
    "_ = print_sparsity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the pruning & finetuning here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.698, 1)]\n",
      "Ratio: 0.698, Epoch: 1\n",
      "Sparsity: 69.837%\n",
      "\n",
      "Finetune epoch: 40 Acc: 88.562 (88562/100000)\n",
      "Test Acc: 85.150 (8515/10000)\n",
      "---------- new score 85.15 ---------- \n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "# (ratio, epoch)\n",
    "sca = [(0.982, 20), (0.983, 20), (0.985, 20)]\n",
    "# tar, bse = 0.97, 5\n",
    "# for idx in range(6):\n",
    "#     sca.append((tar*idx/5,idx*idx+5))\n",
    "print(sca)\n",
    "idx, acc, edx = 1, 1000, 0\n",
    "for (ratio, epoch) in sca:\n",
    "    masks = prune(ratio)\n",
    "    print(f\"Ratio: {ratio}, Epoch: {epoch}\")\n",
    "    _ = print_sparsity()\n",
    "    edx += 1\n",
    "    if (edx == 2):\n",
    "        acc = 0\n",
    "    for epoch in range(1, epoch+1):\n",
    "        finetune(idx)\n",
    "        idx += 1\n",
    "        nacc = test()\n",
    "        if (nacc >= acc):\n",
    "            print(\"-\"*10, f\"new score {nacc}\", \"-\"*10, \"\\nSaving model...\")\n",
    "            acc = nacc\n",
    "        torch.save(net.state_dict(), './checkpoint/resnet18_pruned_{idx}.t7')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'll use the following functions to evaluate your pruned model\n",
    "## Make sure your saved checkpoint can run this on 414 server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_checkpoint():\n",
    "    #resnet18_pruned will be changed to resnet18_fine_StudentID or resnet18_coarse_StudentID during evaluation\n",
    "    path = \"./checkpoint/resnet18_pruned.t7\"\n",
    "    net = model.resnet18()\n",
    "    net.load_state_dict(torch.load(path))\n",
    "    accuracy = test()\n",
    "    sparsity = print_sparsity()\n",
    "    #acc_threshold = 90 for fine-grained, = 85 for coarse-grained\n",
    "    #spar_threshold = 70 for fine=grained, = 25 for coarse-grained\n",
    "    acc_threshold = 85\n",
    "    spar_threshold = 25\n",
    "    if accuracy < acc_threshold or sparsity < spar_threshold:\n",
    "        print(\"failed, accuracy = {:.3f}% sparsity = {:.3f}%\".format(accuracy, sparsity))\n",
    "    else:\n",
    "        print(\"succeeded, accuracy = {:.3f}% sparsity = {:.3f}%\".format(accuracy, sparsity))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_checkpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
