{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18 by pytorch - with pretrain\n",
    "# ColorJitter-contrast every dataset\n",
    "# Use fc(512, 128, 32, 5) \n",
    "# With Batch Normalization\n",
    "# With freezen\n",
    "# epochs : 50\n",
    "\n",
    "# optim : Adam\n",
    "\n",
    "# test_acc : \n",
    "# val_acc : \n",
    "# model_path : ResNet18v2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH =  \"./test\"\n",
    "TRAIN_DATA_PATH = \"./train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.ConcatDataset object at 0x7f965ee866d8>\n"
     ]
    }
   ],
   "source": [
    "# data transform, you can add different transform methods\n",
    "img_size = 224\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_orig = transforms.Compose([\n",
    "                                    transforms.Resize((img_size,img_size)),\n",
    "                                    transforms.ColorJitter(contrast = 0.5),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "train_aug0 = transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(224),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ColorJitter(contrast = 0.5),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "train_aug1 = transforms.Compose([\n",
    "                                    transforms.Resize((img_size,img_size)),\n",
    "                                    transforms.RandomAffine(0,scale=(0.7,1.2)),\n",
    "                                    transforms.ColorJitter(contrast = 0.5),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "train_aug2 = transforms.Compose([\n",
    "                                    transforms.Resize((img_size,img_size)),\n",
    "                                    transforms.transforms.RandomRotation(25),\n",
    "                                    transforms.ColorJitter(contrast = 0.5),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "train_aug3 = transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(224),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ColorJitter(contrast = 0.5),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "train_aug5 = transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(224),\n",
    "                                    transforms.RandomVerticalFlip(),\n",
    "                                    transforms.ColorJitter(contrast = 0.5),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                                    transforms.Resize((img_size,img_size)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "dataset_orig = datasets.ImageFolder(root=TRAIN_DATA_PATH,transform=train_orig)\n",
    "dataset_aug0 = datasets.ImageFolder(root=TRAIN_DATA_PATH,transform=train_aug0)\n",
    "dataset_aug1 = datasets.ImageFolder(root=TRAIN_DATA_PATH,transform=train_aug1)\n",
    "dataset_aug2 = datasets.ImageFolder(root=TRAIN_DATA_PATH,transform=train_aug2)\n",
    "dataset_aug3 = datasets.ImageFolder(root=TRAIN_DATA_PATH,transform=train_aug3)\n",
    "dataset_aug5 = datasets.ImageFolder(root=TRAIN_DATA_PATH,transform=train_aug5)\n",
    "\n",
    "\n",
    "dataset_list = [dataset_orig, dataset_aug0, dataset_aug1, dataset_aug2, dataset_aug3, dataset_aug5]\n",
    "\n",
    "dataset = data.ConcatDataset(dataset_list)\n",
    "\n",
    "\n",
    "test_data = datasets.ImageFolder(root=TEST_DATA_PATH,transform=test_transform)\n",
    "\n",
    "# spilt data into training and validation\n",
    "TOTAL_SIZE = len(dataset)\n",
    "ratio = 0.9\n",
    "train_len = round(TOTAL_SIZE * ratio)\n",
    "val_len = round(TOTAL_SIZE * (1-ratio))\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n",
    "\n",
    "# data loader, you can choose the input arguments by yourself\n",
    "\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True,  num_workers=4)\n",
    "val_data_loader = data.DataLoader(val_dataset, batch_size=128, shuffle=True,  num_workers=4)\n",
    "test_data_loader  = data.DataLoader(test_data, batch_size=8, shuffle=False, num_workers=4) \n",
    "\n",
    "print(dataset)\n",
    "#print(dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8888\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): ResNet(\n",
      "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace)\n",
      "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "          (layer1): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (layer2): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (layer3): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (layer4): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "          (fc): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): Linear(in_features=128, out_features=32, bias=True)\n",
      "    )\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): Linear(in_features=32, out_features=5, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ResNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "          (fc): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): Linear(in_features=32, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "num = model.fc.in_features\n",
    "model.fc = nn.Linear(num, 128)\n",
    "model = nn.Sequential(model, nn.BatchNorm1d(128))\n",
    "model = nn.Sequential(model, nn.Linear(128, 32))\n",
    "model = nn.Sequential(model, nn.BatchNorm1d(32))\n",
    "model = nn.Sequential(model, nn.Linear(32, 5))\n",
    "\n",
    "print(model)\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  0.001\n",
      "Epoch:  1 |Train Loss:  0.8834 |Train Acc:71.3209 |Val Loss:  1.0329 |Val Acc:60.1215\n",
      "-------------saving model--------------\n",
      "2  0.001\n",
      "Epoch:  2 |Train Loss:  0.6074 |Train Acc:79.2529 |Val Loss:  0.9688 |Val Acc:66.1943\n",
      "-------------saving model--------------\n",
      "3  0.001\n",
      "Epoch:  3 |Train Loss:  0.5258 |Train Acc:80.4793 |Val Loss:  0.6177 |Val Acc:74.5951\n",
      "-------------saving model--------------\n",
      "4  0.001\n",
      "Epoch:  4 |Train Loss:  0.4440 |Train Acc:83.2696 |Val Loss:  0.6002 |Val Acc:77.3279\n",
      "-------------saving model--------------\n",
      "5  0.001\n",
      "Epoch:  5 |Train Loss:  0.3813 |Train Acc:85.6436 |Val Loss:  0.7131 |Val Acc:75.4049\n",
      "6  0.001\n",
      "Epoch:  6 |Train Loss:  0.3547 |Train Acc:86.5549 |Val Loss:  0.4714 |Val Acc:82.0850\n",
      "-------------saving model--------------\n",
      "7  0.001\n",
      "Epoch:  7 |Train Loss:  0.3243 |Train Acc:87.8038 |Val Loss:  0.5561 |Val Acc:78.9474\n",
      "8  0.001\n",
      "Epoch:  8 |Train Loss:  0.3171 |Train Acc:87.7700 |Val Loss:  0.3557 |Val Acc:85.5263\n",
      "-------------saving model--------------\n",
      "9  0.001\n",
      "Epoch:  9 |Train Loss:  0.2805 |Train Acc:89.0302 |Val Loss:  0.6253 |Val Acc:76.9231\n",
      "10  0.001\n",
      "Epoch: 10 |Train Loss:  0.2765 |Train Acc:88.7714 |Val Loss:  0.4571 |Val Acc:82.7935\n",
      "11  0.001\n",
      "Epoch: 11 |Train Loss:  0.2650 |Train Acc:89.2552 |Val Loss:  0.3113 |Val Acc:87.2470\n",
      "-------------saving model--------------\n",
      "12  0.001\n",
      "Epoch: 12 |Train Loss:  0.2352 |Train Acc:90.2903 |Val Loss:  0.5777 |Val Acc:78.7449\n",
      "13  0.001\n",
      "Epoch: 13 |Train Loss:  0.2524 |Train Acc:89.7390 |Val Loss:  0.3021 |Val Acc:88.6640\n",
      "-------------saving model--------------\n",
      "14  0.001\n",
      "Epoch: 14 |Train Loss:  0.2320 |Train Acc:90.2228 |Val Loss:  0.3219 |Val Acc:86.3360\n",
      "15  0.001\n",
      "Epoch: 15 |Train Loss:  0.2070 |Train Acc:91.2916 |Val Loss:  0.3166 |Val Acc:87.1457\n",
      "16  0.001\n",
      "Epoch: 16 |Train Loss:  0.2446 |Train Acc:89.7727 |Val Loss:  0.4998 |Val Acc:81.9838\n",
      "17  0.001\n",
      "Epoch: 17 |Train Loss:  0.2057 |Train Acc:90.9991 |Val Loss:  0.2475 |Val Acc:89.7773\n",
      "-------------saving model--------------\n",
      "18  0.001\n",
      "Epoch: 18 |Train Loss:  0.2003 |Train Acc:91.4829 |Val Loss:  0.2563 |Val Acc:88.4615\n",
      "19  0.001\n",
      "Epoch: 19 |Train Loss:  0.1922 |Train Acc:91.9779 |Val Loss:  0.3339 |Val Acc:85.4251\n",
      "20  0.001\n",
      "Epoch: 20 |Train Loss:  0.1981 |Train Acc:91.3254 |Val Loss:  0.2761 |Val Acc:88.9676\n",
      "21  0.001\n",
      "Epoch: 21 |Train Loss:  0.2022 |Train Acc:91.2804 |Val Loss:  0.2910 |Val Acc:88.1579\n",
      "22  0.001\n",
      "Epoch: 22 |Train Loss:  0.1993 |Train Acc:91.1229 |Val Loss:  0.2879 |Val Acc:88.1579\n",
      "23  0.001\n",
      "Epoch: 23 |Train Loss:  0.1790 |Train Acc:92.1242 |Val Loss:  0.3901 |Val Acc:84.6154\n",
      "24  0.001\n",
      "Epoch: 24 |Train Loss:  0.1894 |Train Acc:91.6629 |Val Loss:  0.2574 |Val Acc:89.7773\n",
      "25  0.001\n",
      "Epoch: 25 |Train Loss:  0.1807 |Train Acc:92.0117 |Val Loss:  0.2600 |Val Acc:88.6640\n",
      "26  0.001\n",
      "Epoch: 26 |Train Loss:  0.1699 |Train Acc:92.5180 |Val Loss:  0.2247 |Val Acc:89.3725\n",
      "-------------saving model--------------\n",
      "27  0.001\n",
      "Epoch: 27 |Train Loss:  0.1611 |Train Acc:92.6305 |Val Loss:  0.2924 |Val Acc:87.9555\n",
      "28  0.001\n",
      "Epoch: 28 |Train Loss:  0.1806 |Train Acc:91.9217 |Val Loss:  0.2880 |Val Acc:86.8421\n",
      "29  0.001\n",
      "Epoch: 29 |Train Loss:  0.1625 |Train Acc:92.6418 |Val Loss:  0.4284 |Val Acc:84.7166\n",
      "30  0.001\n",
      "Epoch: 30 |Train Loss:  0.1613 |Train Acc:92.8443 |Val Loss:  0.2194 |Val Acc:90.2834\n",
      "-------------saving model--------------\n",
      "31  0.001\n",
      "Epoch: 31 |Train Loss:  0.1691 |Train Acc:92.0455 |Val Loss:  0.3915 |Val Acc:85.7287\n",
      "32  0.001\n",
      "Epoch: 32 |Train Loss:  0.1728 |Train Acc:92.3155 |Val Loss:  0.2589 |Val Acc:89.0688\n",
      "33  0.001\n",
      "Epoch: 33 |Train Loss:  0.1521 |Train Acc:93.0468 |Val Loss:  0.3944 |Val Acc:87.1457\n",
      "34  0.001\n",
      "Epoch: 34 |Train Loss:  0.1584 |Train Acc:92.5518 |Val Loss:  0.2154 |Val Acc:90.2834\n",
      "-------------saving model--------------\n",
      "35  0.001\n",
      "Epoch: 35 |Train Loss:  0.1491 |Train Acc:92.6305 |Val Loss:  0.2968 |Val Acc:87.6518\n",
      "36  0.001\n",
      "Epoch: 36 |Train Loss:  0.1647 |Train Acc:92.1580 |Val Loss:  0.3675 |Val Acc:85.2227\n",
      "37  0.001\n",
      "Epoch: 37 |Train Loss:  0.1607 |Train Acc:92.6418 |Val Loss:  0.2310 |Val Acc:89.8785\n",
      "38  0.001\n",
      "Epoch: 38 |Train Loss:  0.1528 |Train Acc:92.8443 |Val Loss:  0.2999 |Val Acc:89.0688\n",
      "39  0.001\n",
      "Epoch: 39 |Train Loss:  0.1493 |Train Acc:92.9343 |Val Loss:  0.3418 |Val Acc:85.6275\n",
      "40  0.001\n",
      "Epoch: 40 |Train Loss:  0.1545 |Train Acc:92.7430 |Val Loss:  0.1954 |Val Acc:90.8907\n",
      "-------------saving model--------------\n",
      "41  0.001\n",
      "Epoch: 41 |Train Loss:  0.1424 |Train Acc:92.9343 |Val Loss:  0.2054 |Val Acc:90.5870\n",
      "42  0.001\n",
      "Epoch: 42 |Train Loss:  0.1445 |Train Acc:93.3731 |Val Loss:  0.3474 |Val Acc:87.0445\n",
      "43  0.001\n",
      "Epoch: 43 |Train Loss:  0.1444 |Train Acc:93.3281 |Val Loss:  0.1980 |Val Acc:91.3968\n",
      "44  0.001\n",
      "Epoch: 44 |Train Loss:  0.1531 |Train Acc:92.3267 |Val Loss:  0.2667 |Val Acc:88.4615\n",
      "45  0.001\n",
      "Epoch: 45 |Train Loss:  0.1471 |Train Acc:93.1368 |Val Loss:  0.2184 |Val Acc:89.6761\n",
      "46  0.001\n",
      "Epoch: 46 |Train Loss:  0.1338 |Train Acc:93.5756 |Val Loss:  0.1760 |Val Acc:91.7004\n",
      "-------------saving model--------------\n",
      "47  0.001\n",
      "Epoch: 47 |Train Loss:  0.1525 |Train Acc:92.4505 |Val Loss:  0.2145 |Val Acc:91.1943\n",
      "48  0.001\n",
      "Epoch: 48 |Train Loss:  0.1580 |Train Acc:92.9118 |Val Loss:  0.2658 |Val Acc:88.8664\n",
      "49  0.001\n",
      "Epoch: 49 |Train Loss:  0.1370 |Train Acc:93.0918 |Val Loss:  0.1970 |Val Acc:91.0931\n",
      "50  0.001\n",
      "Epoch: 50 |Train Loss:  0.1349 |Train Acc:93.6656 |Val Loss:  0.2701 |Val Acc:88.5628\n"
     ]
    }
   ],
   "source": [
    "def set_grad(model, is_extract):\n",
    "    if is_extract:\n",
    "        for param in list(model.parameters())[:-4]:\n",
    "            param.requires_grad = False\n",
    "#set_grad(model, True)\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "#         print(\"\\t\",name)\n",
    "\n",
    "learning_rate=1e-2\n",
    "\n",
    "'''optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_sch = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()'''\n",
    "optimizer = optim.Adam(params_to_update, lr=0.001)\n",
    "#lr_sch = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# start training \n",
    "epochs = 50\n",
    "\n",
    "min_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    train_hit = 0\n",
    "    val_hit = 0\n",
    "    print(f\"{epoch}  {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    for data, target in train_data_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output=model(data)\n",
    "        \n",
    "        # loss function\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        train_hit += pred.eq(target.data.view_as(pred)).cpu().sum().item() \n",
    "\n",
    "\n",
    "        # do back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data, target in val_data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_val_loss += F.cross_entropy(output, target).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            val_hit += pred.eq(target.data.view_as(pred)).cpu().sum().item() \n",
    "    \n",
    "    avg_train_loss = total_train_loss/len(train_data_loader)\n",
    "    avg_val_loss   = total_val_loss/len(val_data_loader)\n",
    "    \n",
    "    print('Epoch:%3d'%epoch\n",
    "        , '|Train Loss:%8.4f'%(avg_train_loss)\n",
    "        , '|Train Acc:%3.4f'%(train_hit/len(train_data_loader.dataset)*100.0)\n",
    "        , '|Val Loss:%8.4f'%(avg_val_loss)\n",
    "        , '|Val Acc:%3.4f'%(val_hit/len(val_data_loader.dataset)*100.0))\n",
    "    \n",
    "    if avg_val_loss < min_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "        print(\"-------------saving model--------------\")\n",
    "        # save the model\n",
    "        torch.save(model, \"ResNet18v2.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the model so that you don't need to train the model again\n",
    "test_model = torch.load(\"ResNet18v2.pth\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,data_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        correct = 0\n",
    "        bs = test_data_loader.batch_size\n",
    "        result = []\n",
    "        for i, (data, target) in enumerate(test_data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            arr = pred.data.cpu().numpy()\n",
    "            for j in range(pred.size()[0]):\n",
    "                file_name = test_data.samples[i*bs+j][0].split('/')[-1]\n",
    "                result.append((file_name,pred[j].cpu().numpy()[0]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(test_model,test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('ID_result_ad_18_diflr.csv','w') as f:\n",
    "    f.write('ID,label\\n')\n",
    "    for data in result:\n",
    "        f.write(data[0]+','+str(data[1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
