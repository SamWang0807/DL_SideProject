{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify path for training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH =  \"./test\"\n",
    "TRAIN_DATA_PATH = \"./train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dataset and training/validation data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1646\n",
      "    Root Location: ./train\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "{'actinic keratosis': 0, 'basal cell carcinoma': 1, 'melanoma': 2, 'nevus': 3, 'pigmented benign keratosis': 4}\n"
     ]
    }
   ],
   "source": [
    "# data transform, you can add different transform methods\n",
    "img_size = 224\n",
    "train_transform = transforms.Compose([\n",
    "                                    transforms.Resize((img_size,img_size)),\n",
    "                                    transforms.ToTensor()\n",
    "                                    ])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                    transforms.ToTensor()\n",
    "                                    ])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=TRAIN_DATA_PATH,transform=train_transform)\n",
    "test_data = datasets.ImageFolder(root=TEST_DATA_PATH,transform=test_transform)\n",
    "\n",
    "# spilt data into training and validation\n",
    "TOTAL_SIZE = len(dataset)\n",
    "ratio = 0.1\n",
    "train_len = round(TOTAL_SIZE * ratio)\n",
    "val_len = round(TOTAL_SIZE * (1-ratio))\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n",
    "\n",
    "# data loader, you can choose the input arguments by yourself\n",
    "\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True,  num_workers=4)\n",
    "val_data_loader = data.DataLoader(val_dataset, batch_size=32, shuffle=True,  num_workers=4)\n",
    "test_data_loader  = data.DataLoader(test_data, batch_size=8, shuffle=False, num_workers=4) \n",
    "\n",
    "print(dataset)\n",
    "print(dataset.class_to_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your Resnet18 model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "### You can use any training methods you learn (ex:lr decay, weight decay.....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model =\n",
    "model.to(device=device)\n",
    "\n",
    "learning_rate=\n",
    "optimizer =\n",
    "\n",
    "# start training \n",
    "epochs = \n",
    "\n",
    "min_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    train_hit = 0\n",
    "    val_hit = 0\n",
    "    \n",
    "    for data, target in train_data_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output=model(data)\n",
    "        \n",
    "        # loss function\n",
    "        loss = xxxxxx(output, target)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        train_hit += pred.eq(target.data.view_as(pred)).cpu().sum().item() \n",
    "\n",
    "\n",
    "        # do back propagation\n",
    "        \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data, target in val_data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_val_loss += F.cross_entropy(output, target).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            val_hit += pred.eq(target.data.view_as(pred)).cpu().sum().item() \n",
    "    \n",
    "    avg_train_loss = total_train_loss/len(train_data_loader)\n",
    "    avg_val_loss   = total_val_loss/len(val_data_loader)\n",
    "    \n",
    "    print('Epoch:%3d'%epoch\n",
    "        , '|Train Loss:%8.4f'%(avg_train_loss)\n",
    "        , '|Train Acc:%3.4f'%(train_hit/len(train_data_loader.dataset)*100.0)\n",
    "        , '|Val Loss:%8.4f'%(avg_val_loss)\n",
    "        , '|Val Acc:%3.4f'%(val_hit/len(val_data_loader.dataset)*100.0))\n",
    "    \n",
    "    if avg_val_loss < min_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "        print(\"-------------saving model--------------\")\n",
    "        # save the model\n",
    "        torch.save(model, \"model.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the model so that you don't need to train the model again\n",
    "test_model = torch.load(\"model.pth\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,data_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        correct = 0\n",
    "        bs = test_data_loader.batch_size\n",
    "        result = []\n",
    "        for i, (data, target) in enumerate(test_data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            arr = pred.data.cpu().numpy()\n",
    "            for j in range(pred.size()[0]):\n",
    "                file_name = test_data.samples[i*bs+j][0].split('/')[-1]\n",
    "                result.append((file_name,pred[j].cpu().numpy()[0]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(test_model,test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('ID_result.csv','w') as f:\n",
    "    f.write('ID,label\\n')\n",
    "    for data in result:\n",
    "        f.write(data[0]+','+str(data[1])+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
